{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fdda8f3",
   "metadata": {},
   "source": [
    "### 1. 웹스크래핑 연습문제 (질문1, 질문2 통합)\n",
    "* Daum 뉴스기사 제목 스크래핑하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "852368e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#section_dict = {100:'정치',101:'경제',102:'사회',103:'생활/문화',104:'세계',105:'IT/과학'}\n",
    "section_dict = {'기후/환경':'climate','사회':'society','경제':'economy','정치':'politics',\n",
    "                '국제':'world','문화':'culture','생활':'life','IT/과학':'tech','인물':'people'}\n",
    "\n",
    "def print_news(section_name):  #print_new(103,'생활/문화') \n",
    "  sid = section_dict.get(section_name,'society')\n",
    "  url = f'https://news.daum.net/{sid}'\n",
    "  print(f'{section_name} 뉴스 {url}')\n",
    "  req_header = {\n",
    "    'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'\n",
    "}\n",
    "  res = requests.get(url, headers=req_header)\n",
    "  res.encoding = 'utf-8'\n",
    "\n",
    "  if res.ok:\n",
    "    soup = BeautifulSoup(res.text,'html.parser')\n",
    "\n",
    "    # CSS 선택자를 사용해서 a tag 목록 가져오기\n",
    "    li_tags = soup.select('ul.list_newsheadline2 li')\n",
    "\n",
    "    for li_tag in li_tags:\n",
    "        a_tag = li_tag.find('a')\n",
    "        link = a_tag['href']\n",
    "        print(link)\n",
    "\n",
    "        strong_tag = li_tag.select_one('div.cont_thumb strong.tit_txt')\n",
    "\n",
    "        if strong_tag:\n",
    "                    title = strong_tag.text.strip()\n",
    "                    print(f'제목: {title}')\n",
    "                    print(f'링크: {link}')\n",
    "                    print('-' * 30)\n",
    "        \n",
    "  else :\n",
    "  # 응답(response)이 Error 이면 status code 출력    \n",
    "    print(f'Error Code = {res.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c235faee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_news(\"인물\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41c7625",
   "metadata": {},
   "source": [
    "### 2. 웹 스크래핑 연습문제\n",
    "* 2-1. Nate 뉴스기사 제목 스크래핑하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8d661ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin # 1) 경로 결합을 위한 urljoin\n",
    "from IPython.display import Image, display # 2) 이미지 출력을 위한 Image, display\n",
    "\n",
    "# 섹션별 경로와 mid 값 정의\n",
    "section_dict = {\n",
    "    '최신': ('recent', 'n0100'),\n",
    "    '정치': ('section', 'n0200'),\n",
    "    '경제': ('section', 'n0300'),\n",
    "    '사회': ('section', 'n0400'),\n",
    "    '생활/문화': ('section', 'n0500'),\n",
    "    'IT/과학': ('section', 'n0600')\n",
    "}\n",
    "\n",
    "def print_news(section_name):\n",
    "    # 딕셔너리 정보 추출 및 URL 생성\n",
    "    path, mid = section_dict.get(section_name, ('recent', 'n0100'))\n",
    "    url = f'https://news.nate.com/{path}?mid={mid}'\n",
    "    print(f'### {section_name} 뉴스 목록: {url} ###\\n')\n",
    "    \n",
    "    req_header = {\n",
    "        'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    res = requests.get(url, headers=req_header)\n",
    "    res.encoding = res.apparent_encoding\n",
    "\n",
    "    if res.ok:\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        # [해결포인트] 보내주신 HTML 구조인 a.lt1를 직접 찾거나, 일반적인 li 구조를 모두 찾습니다.\n",
    "        # 선택자가 하나라도 맞으면 데이터를 가져옵니다.\n",
    "        news_items = soup.select('a.lt1, .mlist2 li, .mduSubjectList li')\n",
    "\n",
    "        if not news_items:\n",
    "            print(\"데이터를 찾지 못했습니다. 선택자를 확인해주세요.\")\n",
    "            return\n",
    "\n",
    "        for item in news_items:\n",
    "            # 1) a 태그(기사 링크) 및 제목 추출\n",
    "            # item 자체가 a 태그일 수도 있고(a.lt1), li일 수도 있으므로 분기 처리\n",
    "            if item.name == 'a':\n",
    "                a_tag = item\n",
    "            else:\n",
    "                a_tag = item.select_one('a')\n",
    "            \n",
    "            if not a_tag: continue\n",
    "\n",
    "            # urljoin을 사용해 기사 링크 생성\n",
    "            link = urljoin(url, a_tag['href'])\n",
    "            \n",
    "            # 제목 추출 (보내주신 h2.tit 포함)\n",
    "            title_tag = item.select_one('h2.tit, strong.tit, span.tb, .tit')\n",
    "            if not title_tag: continue\n",
    "            \n",
    "            title = title_tag.text.strip()\n",
    "\n",
    "            # 3) img 엘리먼트 존재 여부 체크\n",
    "            img_tag = item.select_one('img')\n",
    "\n",
    "            # 출력 순서: Image -> 제목 -> 링크\n",
    "            if img_tag and img_tag.get('src'):\n",
    "                src = img_tag['src']\n",
    "                # 1) urljoin으로 // 경로와 도메인 결합\n",
    "                full_img_url = urljoin(url, src)\n",
    "                if '///' in src:\n",
    "                    src = src.split('///')[-1]\n",
    "                \n",
    "                # 2) Image 클래스와 display 함수로 이미지 출력\n",
    "                display(Image(url=full_img_url, width=150))\n",
    "            else:\n",
    "                # 이미지가 없는 뉴스일 경우\n",
    "                print(\"(이미지 없음)\")\n",
    "            \n",
    "            print(f'제목: {title}')\n",
    "            print(f'링크: {link}')\n",
    "            print('-' * 60)\n",
    "            \n",
    "    else:\n",
    "        print(f'Error Code = {res.status_code}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960060b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_news('경제')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1409c5",
   "metadata": {},
   "source": [
    "### 2-2. 하나의 네이버 웹툰과 1개의 회차에 대한 Image 다운로드 하기 (필수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0298673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "webtoon_url = 'https://comic.naver.com/webtoon/detail?titleId=846643&no=4&week=tue'\n",
    "\n",
    "\n",
    "req_header = {\"referer\": \"https://comic.naver.com/webtoon/detail?titleId=846643&no=4&week=tue\",\n",
    "              'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'}\n",
    "\n",
    "def download_one_episode(title, no, url):\n",
    "    # 1. 저장 디렉토리 설정 (img\\제목\\회차)\n",
    "    # os.path.join을 사용하여 운영체제에 맞는 경로(img/제목/회차)를 생성합니다.\n",
    "    imgdir_name = os.path.join('img', title, str(no))\n",
    "    \n",
    "    # 디렉토리가 없으면 하위 폴더까지 포함하여 생성 (exist_ok=True는 이미 폴더가 있어도 에러를 내지 않습니다)\n",
    "    if not os.path.exists(imgdir_name):\n",
    "        os.makedirs(imgdir_name, exist_ok=True)\n",
    "        print(f\"디렉토리 생성: {imgdir_name}\")\n",
    "\n",
    "res = requests.get(webtoon_url, headers=req_header)\n",
    "if res.ok:\n",
    "    # .jpg 파일명 추출해서 list에 저장하기\n",
    "    # img_url_list = [] #list()\n",
    "    soup = BeautifulSoup(res.text,'html.parser')\n",
    "    print(len(soup.select(\"img[src*='IMAG01']\")))\n",
    "    img_tags = soup.select(\"img[src*='IMAG01']\")\n",
    "    # for img_tag in img_tags:\n",
    "    #     img_url = img_tag['src']\n",
    "    #     img_url_list.append(img_url)\n",
    "    # print(img_url_list)\n",
    "\n",
    "  # List Comprehension Pthonic (쉽게짠거)\n",
    "    img_url_list = [img_tag['src'] for img_tag in img_tags]\n",
    "    print(img_url_list)\n",
    "    imgdir_name = 'img'\n",
    "    if not os.path.isdir(imgdir_name):\n",
    "        os.mkdir(imgdir_name) \n",
    "    for img_url in img_url_list:\n",
    "        res = requests.get(img_url, headers=req_header)\n",
    "        print(res.status_code)\n",
    "        # binary 응답 데이터 가져오기\n",
    "        img_data = res.content\n",
    "        # url에서 파일명만 추출하기\n",
    "        #img/xxxIMGA01.jpg\n",
    "        file_path = os.path.join(imgdir_name,os.path.basename(img_url))\n",
    "        # binday data를 file에 write하기\n",
    "        with open(file_path, 'wb') as file:\n",
    "            print(f'Writing to {file_path}({len(img_data):,}bytes)')\n",
    "            file.write(img_data)\n",
    "\n",
    "else :\n",
    "  # 응답(response)이 Error 이면 status code 출력    \n",
    "    print(f'Error Code = {res.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01866a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
